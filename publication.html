<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Feng Liu (he/him) -- Assistant Professor at The University of Melbourne</title>
</head>

<body>
<h1 style="padding-left: 0.5em">Feng Liu (he/him) -- Assistant Professor at The University of Melbourne</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
    <div class="menu-item"><a href="focus.html">Research Focus</a></div>
    <div class="menu-item"><a href="group.html">Research Group</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="publication.html">Publications</a></div>
    <div class="menu-item"><a href="service.html">Professional Services</a></div>
    <div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
    <div class="menu-item"><a href="join.html">Join Us</a></div>
</td>
<td id="layout-content">

    <h1 style="margin-top: 0em">Publications</h1><br>
	Currently, I research the trustworthy machine learning (mainly focus on transfer learning and adversarial machine learning) and two-sample testing (a fundamental problem in machine learning and statistics). Previously (2013-2016), I researched the time series prediction using neural networks. In the following, <sup>&#8224</sup> represents equal contribution, and ✉ represents corresponding author.
    
    <p>[<a href="#conference">Selected Conference Papers</a>,
        <a href="#journal">Selected Journal Articles</a>,
        <a href="#thesis">Theses</a> ]</p>
	
    <div>
        <h2><hr>Working Papers</h2>
        <ol>
			<!-- <li><p>
                <b>F. Liu</b>, J. Lu, A. Liu and G. Zhang.<br>
                Discrepancy of Diverse Subsets for Distribution Comparison.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i></a>, Under Review, 2018 (ERA <b>A*</b>).
            </p></li>
            <li><p>
                <b>F. Liu</b>, Z. Fang, J. Lu and G. Zhang.<br>
                Maximum Local Discrepancy: A Non-parametric Two Sample Test.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank"><i>IEEE Transactions on Neural Networks and Learning Systems</i></a>, Under Review, 2019 (ERA&CORE <b>A*</b>).
            </p></li> 
            <li><p>
                <b>F. Liu</b>, J. Lu and G. Zhang.<br>
                A Knowledge-Ensemble Framework for Heterogeneous Unsupervised Domain Adaptation.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank"><i>IEEE Transactions on Neural Networks and Learning Systems</i></a>, Under Review (ERA&CORE <b>A*</b>).
            </p></li>-->
            <li><p>
                R. Gao<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, K. Zhou, G. Niu, B. Han and J. Cheng.<br>
                Local Reweighting for Adversarial Training.<br>
                [ <a href="https://arxiv.org/pdf/2106.15776.pdf" target="_blank">arXiv</a> ]
            </p></li>
            
        </ol>
    </div>

    <div>
        <h2><hr><a name="conference"></a>Selected Conference Papers</h2>
        <ol>
        <li><p>
                X. Xu, J. Zhang, <b>F. Liu</b>, M. Sugiyama, M. Kankanhalli.<br>
                Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection.<br>
            In <a href="https://neurips.cc/Conferences/2023" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2023)</i></a>, to appear (CORE <b>A*</b>).<br>
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">arXiv</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">CODE</a>]
                [ <font color="green"><b> Spotlight </b></font> ]
        </p></li>
        <li><p>
                X. Xu, J. Zhang, <b>F. Liu</b>, M. Sugiyama, M. Kankanhalli.<br>
                Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization.<br>
            In <a href="https://neurips.cc/Conferences/2023" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2023)</i></a>, to appear (CORE <b>A*</b>).<br>
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">arXiv</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                H. Zheng, Q. Wang, Z. Fang, X. Xia, <b>F. Liu</b>, T. Liu, B. Han.<br>
                Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources.<br>
            In <a href="https://neurips.cc/Conferences/2023" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2023)</i></a>, to appear (CORE <b>A*</b>).<br>
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">arXiv</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                Q. Wang, Z. Fang, Y. Zhang, <b>F. Liu</b>, Y. Li, B. Han.<br>
                Learning to Augment Distributions for Out-of-distribution Detection.<br>
            In <a href="https://neurips.cc/Conferences/2023" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2023)</i></a>, to appear (CORE <b>A*</b>).<br>
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">arXiv</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                K. Liu, <b>F. Liu</b>, H. Wang, N. Ma, J. Bu, B. Han.<br>
                Partition Speeds Up Learning Implicit Neural Representations Based on Exponential-Increase Hypothesis.<br>
            In <a href="https://icml.cc/Conferences/2023" target="_blank"><i>International Conference on Computer Vision (ICCV 2023)</i></a>, to appear (CORE <b>A*</b>).<br>
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">arXiv</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                R. Dong<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, H. Chi, T. Liu, M. Gong, G. Niu, M. Sugiyama, B. Han.<br>
                Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation.<br>
            In <a href="https://icml.cc/Conferences/2023" target="_blank"><i>International Conference on Machine Learning (ICML 2023)</i></a>, Hawaii, US (CORE <b>A*</b>).<br>
                [ <a href="https://openreview.net/pdf?id=PJzjHAnoVp" target="_blank">Openreview</a> ]
                [ <a href="https://github.com/Ruijiang97/DEG-Net" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                S. Zhang<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, J. Yang, Y. Yang,  C. Li, B. Han, M. Tan.<br>
                Detecting Adversarial Data by Probing Multiple Perturbations Using Expected Perturbation Score.<br>
            In <a href="https://icml.cc/Conferences/2023" target="_blank"><i>International Conference on Machine Learning (ICML 2023)</i></a>, Hawaii, US (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2305.16035.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/ZSHsh98/EPS-AD" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                X. Jiang, <b>F. Liu</b>, Z. Fang, H. Chen, T. Liu, F. Zheng, B. Han.<br>
                Detecting Out-of-distribution Data through In-distribution Class Prior.<br>
            In <a href="https://icml.cc/Conferences/2023" target="_blank"><i>International Conference on Machine Learning (ICML 2023)</i></a>, Hawaii, US (CORE <b>A*</b>).<br>
                [ <a href="https://openreview.net/pdf?id=charggEv8v" target="_blank">Openreview</a> ]
                [ <a href="https://github.com/tmlr-group/class_prior" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                T. Cui, Y. Li, K. Chen, J. Bailey, <b>F. Liu</b>.<br>
                Designing Fair AI Systems: Exploring the Interaction of Explainable AI and Task Objectivity on Users’ Fairness Perception.<br>
                In <a href="https://pacis2023.aisconferences.org/" target="_blank"><i>Pacific Asia Conference on Information Systems (PACIS 2023)</i></a>, Nanchang, China<br>
                [ <a href="https://aisel.aisnet.org/pacis2023/161/" target="_blank">Link</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">Social Science Theory</a> ]
        </p></li>
        <li><p>
                Y. Song, T. Cui, <b>F. Liu</b>.<br>
                Designing Fair AI Systems: How Explanation Specificity Influences Users' Perceived Fairness and Trusting Intentions.<br>
                In <a href="https://ecis2023.no/" target="_blank"><i>European Conference on Information Systems (ECIS 2023)</i></a>, Kristiansand, Norway<br>
                [ <a href="https://aisel.aisnet.org/ecis2023_rip/7/" target="_blank">Link</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">Social Science Theory</a> ]
                [ <font color="green"><b> Best RiP Paper Runner-up Award (2nd) </b></font> ]
        </p></li>
        <li><p>
                Q. Wang, J. Ye, <b>F. Liu</b>, Q. Dai, M. Kalander, T. Liu, J. Hao, B. Han.<br>
                Out-of-distribution Detection with Implicit Outlier Transformation.<br>
                In <a href="https://iclr.cc/Conferences/2023" target="_blank"><i>International Conference on Learning Representations (ICLR 2023)</i></a>, Kigali, Rwanda (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2303.05033.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/QizhouWang/DOE" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                Z. Fang, Y. Li, J. Lu, J. Dong, B. Han, <b>F. Liu</b>.<br>
                Is Out-of-distribution Detection Learnable?<br>
                In <a href="https://neurips.cc/Conferences/2022" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2022)</i></a>, online (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2210.14707.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">ML Theory</a> ]
                [ <font color="green"><b> Outstanding Paper Award </b></font> ] <font color="green">(outstanding papers:acceptance:submissions=13:2672:10411)</font>

        </p></li>
        <li><p>
                Q. Wang<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, Y. Zhang, J. Zhang, C. Gong, T. Liu, B. Han.<br>
                Watermarking for Out-of-distribution Detection.<br>
                In <a href="https://neurips.cc/Conferences/2022" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2022)</i></a>, online (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2210.15198.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/QizhouWang/watermarking" target="_blank">CODE</a>]
                [ <font color="green"><b> Spotlight </b></font> ]
        </p></li>
        
        <li><p>
                X. Peng<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, J. Zhang, L. Lan, J. Ye, T. Liu, B. Han.<br>
                Bilateral Dependency Optimization: Defending Against Model-inversion Attacks.<br>
            In <a href="https://kdd.org/kdd2022/" target="_blank"><i>ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2022)</i></a>, online (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2206.05483.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/xpeng9719/Defend_MI" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                R. Gao, J. Wang, K. Zhou, <b>F. Liu</b>, B. Xie, G. Niu, B. Han, J. Cheng.<br>
                Fast and Reliable Evaluation of Adversarial Robustness with Minimum-Margin Attack.<br>
            In <a href="https://icml.cc/Conferences/2022" target="_blank"><i>International Conference on Machine Learning (ICML 2022)</i></a>, online (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2206.07314.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/Sjtubrian/MM-attack" target="_blank">CODE</a>]
        </p></li>
        <li><p>
                X. Xu<sup>&#8224</sup>, J. Zhang<sup>&#8224</sup>, <b>F. Liu</b>, M. Sugiyama, M. Kankanhalli.<br>
                Adversarial Attacks and Defense for Non-Parametric Two-Sample Tests.<br>
            In <a href="https://icml.cc/Conferences/2022" target="_blank"><i>International Conference on Machine Learning (ICML 2022)</i></a>, online (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2202.03077.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/GodXuxilie/Robust-TST" target="_blank">CODE</a>]
        </p></li>
	    <li><p>
                H. Chi<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, B. Han, W. Yang, L. Lan, T. Liu, G. Niu, M. Zhou and M. Sugiyama.<br>
                Meta Discovery: Learning to Discover Novel Classes given Very Limited Data.<br>
		In <a href="https://iclr.cc/Conferences/2022" target="_blank"><i>International Conference on Learning Representations (ICLR 2022)</i></a>, online, 2022 (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2102.04002.pdf" target="_blank">arXiv</a> ]
		[ <a href="https://github.com/Haoang97/MEDI" target="_blank">CODE</a>]
		[ <font color="green"><b> Spotlight </b></font> ] <font color="green">(spotlights:acceptance:submissions=176:1095:3391)</font>
            </p></li>
            <li><p>
                <b>F. Liu</b><sup>&#8224</sup>, W. Xu<sup>&#8224</sup>, J. Lu, D. J. Sutherland.<br>
                Meta Two-Sample Testing: Learning Kernels for Testing with Limited Data.<br>
                In <a href="https://neurips.cc/Conferences/2021" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2021)</i></a>, online, 2021 (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2106.07636.pdf" target="_blank">arXiv</a> ]
		        [ <a href="https://github.com/fengliu90/MetaTesting" target="_blank">CODE</a>]
            </p></li>
            <li><p>
                H. Chi<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, W. Yang, L. Lan, T. Liu, B. Han, W. Cheung and J. T. Kwok.<br>
                TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation.<br>
                In <a href="https://neurips.cc/Conferences/2021" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2021)</i></a>, online, 2021 (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2106.06326" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/Haoang97/TOHAN" target="_blank">CODE</a>]
                [ <font color="green"><b> Spotlight </b></font> ] <font color="green">(spotlights:acceptance:submissions=260:2372:9122)</font>
            </p></li>
            <li><p>
                Q. Wang<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, B. Han, T. Liu, C. Gong, M. Zhou and M. Sugiyama.<br>
                Probabilistic Margins for Instance Reweighting in Adversarial Training.<br>
                In <a href="https://neurips.cc/Conferences/2021" target="_blank"><i>Advances in Neural Information Processing Systems (NeurIPS 2021)</i></a>, online, 2021 (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2106.07904" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/QizhouWang/MAIL" target="_blank">CODE</a>]
            </p></li>
        	<li><p>
                R. Gao<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, J. Zhang<sup>&#8224</sup>, B. Han, T. Liu, G. Niu and M. Sugiyama.<br>
                Maximum Mean Discrepancy is Aware of Adversarial Attacks.<br>
                In <a href="https://icml.cc/Conferences/2021" target="_blank"><i>International Conference on Machine Learning (ICML 2021)</i></a>, online, 2021 (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/abs/2010.11415" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/Sjtubrian/SAMMD" target="_blank">CODE</a>]
            </p></li>
            <li><p>
                Z. Fang<sup>&#8224</sup>, J. Lu, A. Liu<sup>&#8224</sup>, <b>F. Liu</b>, G. Zhang.<br>
                Learning Bounds for Open-Set Learning.<br>
                In <a href="https://icml.cc/Conferences/2021" target="_blank"><i>International Conference on Machine Learning (ICML 2021)</i></a>, online, 2021 (CORE <b>A*</b>).<br>
		[ <a href="https://arxiv.org/pdf/2106.15792.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/Anjin-Liu/Openset_Learning_AOSR" target="_blank">CODE</a> ]
            </p></li>
        	<li><p>
                L. Zhong<sup>&#8224</sup>, Z. Fang<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, B. Yuan, G. Zhang and J. Lu.<br>
                How does the Combined Risk Affect the Performance of Unsupervised Domain Adaptation Approaches?<br>
                In <a href="https://aaai.org/Conferences/AAAI-21/" target="_blank"><i>AAAI Conference on Artificial Intelligence (AAAI 2021)</i></a>, online, 2021 (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2101.01104.pdf" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/fengliu90/E-MixNet" target="_blank">CODE</a> ]
            </p></li> 
            <li><p>
                <b>F. Liu</b><sup>&#8224</sup>, W. Xu<sup>&#8224</sup>, J. Lu, G. Zhang and A. Gretton, D. J. Sutherland.<br>
                Learning Deep Kernels for Non-parametric Two Sample Test.<br>
				In <a href="https://icml.cc/Conferences/2020" target="_blank"><i>International Conference on Machine Learning (ICML 2020)</i></a>, online, 2020 (CORE <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2002.09116" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/fengliu90/DK-for-TST" target="_blank">CODE</a> ]
            </p></li>
            <li><p>
                Y. Zhang<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, Z. Fang<sup>&#8224</sup>, B. Yuan,  G. Zhang and J. Lu.<br>
                Clarinet: A One-step Approach Towards Budget-friendly Unsupervised Domain Adaptation.<br>
				In <a href="https://ijcai20.org/" target="_blank"><i>International Joint Conference on Artificial Intelligence (IJCAI 2020)</i></a>, online, 2021 (CORE <b>A*</b>).<br>                
                [ <a href="https://arxiv.org/pdf/2007.14612" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/fengliu90/BFUDA" target="_blank">CODE</a> ]
            </p></li>
			<li><p>
                <b>F. Liu</b>, J. Lu, B. Han, G. Niu, G. Zhang and M. Sugiyama.<br>
                Butterfly: A Panacea for All Difficulties in Wildly Unsupervised Domain Adaptation.<br>
				In <a href="https://www.skillsworkshop.ai/" target="_blank"><i>Learning Transferable Skills Workshop</i></a> on <a href="http://nips.cc/Conferences/2019/" target="_blank"><i>Neural Information Processing Systems (NeurIPS 2019 Workshop)</i></a>, Vancouver, Canada, 2019 (CORE <b>A*</b>).<br>
                [ <a href="https://www.skillsworkshop.ai/uploads/1/2/1/5/121527312/butterfly.pdf" target="_blank">PDF</a> ]
                [ <a href="https://github.com/fengliu90/Butterfly" target="_blank">CODE</a> ]
            </p></li>
            <li><p>
                <b>F. Liu</b>, G. Zhang and J. Lu.<br>
                A Novel Fuzzy Neural Network for Unsupervised Domain Adaptation in Heterogeneous Scenarios.<br>
                In <a href="https://attend.ieee.org/fuzzieee-2019/about/" target="_blank"><i>IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2019)</i></a>,
                New Orleans, US, 2019 (CORE <b>A</b>).<br>
                [ <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858889" target="_blank">link</a> ]
                [ <font color="green"><b> Best Student Paper Award </b></font> ]
            </p></li>
        </ol>
    </div>

    <div>
        <h2><hr><a name="journal"></a>Selected Journal Articles</h2>
        <ol>
            <li><p>
                X. Feng<sup>&#8224</sup>, Z. Yu<sup>&#8224</sup>, H. Fang, H. Jiang, G. Yang, L. Chen, X. Zhou, B. Hu, C. Qin, G. Hu, G. Xing, B. Zhao, Y. Shi, J. Guo, <b>F. Liu</b>, B. Han, B. Zechmann, Y. He<sup>✉</sup>, and F. Liu<sup>✉</sup>.<br>
                Plantorganelle Hunter: An Effective Deep Learning-based Method for Plant Organelle Phenotyping in Electron Microscopy.<br>
                <a href="https://www.nature.com/nplants/" target="_blank"><i>Nature Plants</i></a>, 2023.<br>
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">link</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">CODE</a> ]
            </p></li>
            <li><p>
                C. Zhou<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, C. Gong, R. Zeng, T. Liu, W. Cheung and B. Han.<br>
                KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation.<br>
                <a href="https://jmlr.org/tmlr/" target="_blank"><i>Transactions on Machine Learning Research</i></a>, 2023.<br>
                [ <a href="https://arxiv.org/pdf/2106.06237" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                X. Guo, F. Lin, J. Song, S. Di, L. Lin, Z. Zhong, Z. Wu, X. Wang, Y. Zhang, J. Li, H. Zhang<sup>✉</sup>, <b>F. Liu<sup>✉</sup></b>, C. Yang<sup>✉</sup>, J. Song<sup>✉</sup>.<br>
                Deep Transfer Learning Enables Lesion Tracing of Circulating Tumor Cells.<br>
                <a href="https://www.nature.com/ncomms/" target="_blank"><i>Nature Communications</i></a>, 2022.<br>
                [ <a href="https://www.nature.com/articles/s41467-022-35296-0" target="_blank">link</a> ]
                [ <a href="https://github.com/AsaHIXx/CTCT" target="_blank">CODE</a> ]
            </p></li>
             <li><p>
                Z. Fang, J. Lu<sup>✉</sup>, <b>F. Liu<sup>✉</sup></b>, G. Zhang.<br>
                Semi-supervised Heterogeneous Domain Adaptation: Theory and Algorithms.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank"><i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i></a>, 2022 (ERA <b>A*</b>).<br>
                [ <a href="https://ieeexplore.ieee.org/abstract/document/9695325/" target="_blank">link</a> ]
                [ <a href="https://fengliu90.github.io/publication.html" target="_blank">CODE</a> ]
            </p></li>
        <li><p>
                <b>F. Liu</b>, J. Lu, B. Han, G. Niu, G. Zhang and M. Sugiyama.<br>
                Butterfly: One-step Approach towards Wildly Unsupervised Domain Adaptation.<br>
        <i>Preprint</i>, 2021.<br>
                [ <a href="https://arxiv.org/abs/1905.07720" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/fengliu90/Butterfly" target="_blank">CODE</a> ]
            </p></li>
            
        <li><p>
                L. Zhong<sup>&#8224</sup>, Z. Fang<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, B. Yuan, G. Zhang and J. Lu.<br>
                Bridging the Theoretical Bound and Deep Algorithms for Open Set Domain Adaptation.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank"><i>IEEE Transactions on Neural Networks and Learning Systems</i></a>, 2021 (ERA <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2006.13022" target="_blank">arXiv</a> ]
            </p></li>
        <li><p>
                Y. Zhang<sup>&#8224</sup>, <b>F. Liu</b><sup>&#8224</sup>, Z. Fang<sup>&#8224</sup>,  B. Yuan, G. Zhang and J. Lu.<br>
                Learning from a Complementary-label Source Domain: Theory and Algorithms.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank"><i>IEEE Transactions on Neural Networks and Learning Systems</i></a>, 2021 (ERA <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/2006.13022" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/fengliu90/BFUDA" target="_blank">CODE</a> ]
            </p></li>
            <li><p>
                <b>F. Liu</b>, G. Zhang and J. Lu.<br>
                Multi-source Heterogeneous Unsupervised Domain Adaptation via Fuzzy-relation Neural Networks.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=91" target="_blank"><i>IEEE Transactions on Fuzzy Systems</i></a>, 2020 (ERA <b>A*</b>).<br>
                [ <a href="https://ieeexplore.ieee.org/document/9172137" target="_blank">link</a> ]
            </p></li>
        <li><p>
                <b>F. Liu</b>, G. Zhang and J. Lu.<br>
                Heterogeneous domain adaptation: An unsupervised approach.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank"><i>IEEE Transactions on Neural Networks and Learning Systems</i></a>, 2020 (ERA <b>A*</b>).<br>
                [ <a href="https://arxiv.org/abs/1701.02511" target="_blank">arXiv</a> ]
            </p></li>
            <li><p>
                S. Qin<sup>&#8224</sup>, H. Ding<sup>&#8224</sup>, Y. Wu<sup>&#8224</sup> and <b>F. Liu</b><sup>&#8224</sup>.<br>
                High-dimensional sign-constrained feature selection and grouping.<br>
                <a href="https://www.springer.com/journal/10463" target="_blank"><i>Annals of the Institute of Statistical Mathematics</i></a>, Oct., 2020 (ERA <b>A</b>).<br>
                [ <a href="https://link.springer.com/article/10.1007/s10463-020-00766-z" target="_blank">link</a> ]
            </p></li>
            <li><p>
                Z. Fang, J. Lu, <b>F. Liu</b>, J. Xuan and G. Zhang.<br>
                Open set domain adaptation: Theoretical bound and algorithm.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank"><i>IEEE Transactions on Neural Networks and Learning Systems</i></a>, 2020 (ERA <b>A*</b>).<br>
                [ <a href="https://arxiv.org/pdf/1907.08375" target="_blank">arXiv</a> ]
                [ <a href="https://github.com/fang-zhen/Open-set-domain-adaptation" target="_blank">CODE</a> ]
            </p></li>
            <li><p>
                <b>F. Liu</b>, J. Lu and G. Zhang.<br>
                Unsupervised heterogeneous domain adaptation via shared fuzzy equivalence relations.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=91" target="_blank"><i>IEEE Transactions on Fuzzy Systems</i></a>, vol. 26, no. 6, pp. 3555–-3568, 2018 (ERA <b>A*</b>).<br>
                [ <a href="https://ieeexplore.ieee.org/abstract/document/8359394" target="_blank">link</a> ]
                [ <a href="https://github.com/fengliu90/SFER_code" target="_blank">CODE</a> ]
            </p></li>
            <li><p>
                H. Zuo, J. Lu, G. Zhang and <b>F. Liu</b>.<br>
                Fuzzy transfer learning using an infinite gaussian mixture model and active learning.<br>
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=91" target="_blank"><i>IEEE Transactions on Fuzzy Systems</i></a>, vol. 27, no. 2, pp. 291–-303, 2018 (ERA <b>A*</b>).<br>
                [ <a href="https://ieeexplore.ieee.org/document/8413167" target="_blank">link</a> ]
            </p></li>
            
            
        </ol>
    </div>

    <div>
        <h2><hr><a name="thesis"></a>Theses</h2>
        <ol>
             <li><p>
                <b>Feng Liu</b>.<br>
                <i>Towards Realistic Transfer Learning Methods: Theory and Algorithms.</i><br>
                Doctoral Thesis, Australian Artificial Intelligence Institute, University of Technology Sydney, Australia, November 2020.
            </p></li>
            <li><p>
                <b>Feng Liu</b>.<br>
                <i>Time Series Interpolation and Prediction for the Electricity Market.</i><br>
                Master Thesis, School of Mathematic and Statistics, Lanzhou University, China, June 2015.
            </p></li>
        </ol>
    </div>

</td>
</tr>
</table>
</body>
</html>
